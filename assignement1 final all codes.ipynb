{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "432d196e-6159-4760-9b7b-9ba10040e418",
   "metadata": {},
   "source": [
    "# Intelligent Systems – Assignment 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8fb7ad2-bdc8-4c29-b642-644f59f5f588",
   "metadata": {},
   "source": [
    "### Alexandre Baptista ist1 100514\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d29449a-1df9-466b-bc99-cbfa2e1b045a",
   "metadata": {},
   "source": [
    "This assignment addresses fuzzy modelling of two machine learning tasks: regression and classification. The chosen approach is based on Takagi–Sugeno–Kang (TSK) fuzzy systems, which combine fuzzy rules with local linear models to provide interpretable and flexible predictive models.\n",
    "\n",
    "Two datasets were considered:\n",
    "\n",
    "The Diabetes dataset (from scikit-learn) for regression, where the objective is to predict a quantitative measure of disease progression based on 10 baseline medical variables.\n",
    "\n",
    "The Pima Indians Diabetes dataset (from OpenML) for classification, where the task is to predict the presence of diabetes from 8 clinical features.\n",
    "\n",
    "The models were developed in Python using fuzzy c-means clustering to identify rule antecedents and least-squares estimation for rule consequents, following the teacher’s template. Performance was evaluated using Mean Squared Error (MSE) for regression and Accuracy (ACC) for classification. Additionally, I experimented with alternative consequents (logistic regression per rule) to test whether performance could be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa1a1116-5264-48fb-9283-7a185d3b3992",
   "metadata": {},
   "source": [
    "\n",
    "## Dataset 1: Diabetes Dataset (Regression)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "dad0936a-9741-4876-aea7-922c9a913c71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[REG] Test MSE: 2443.032\n",
      "\n",
      "Regras (centros/sigmas em espaço padronizado):\n",
      "- Regra 1: centro[age≈+0.31σ, sex≈+0.44σ, bmi≈+0.49σ, bp≈+0.33σ, s1≈+0.94σ, s2≈+0.97σ, s3≈-0.62σ, s4≈+1.13σ, s5≈+0.81σ, s6≈+0.62σ] | σ_age=0.85, σ_sex=0.93, σ_bmi=0.86, σ_bp=0.94, σ_s1=0.92, σ_s2=0.98, σ_s3=0.72, σ_s4=0.93, σ_s5=0.84, σ_s6=0.91\n",
      "- Regra 2: centro[age≈+0.22σ, sex≈+0.50σ, bmi≈-0.22σ, bp≈-0.06σ, s1≈-0.37σ, s2≈-0.27σ, s3≈-0.03σ, s4≈-0.25σ, s5≈-0.28σ, s6≈-0.11σ] | σ_age=0.87, σ_sex=0.90, σ_bmi=0.79, σ_bp=0.87, σ_s1=0.78, σ_s2=0.75, σ_s3=0.85, σ_s4=0.72, σ_s5=0.81, σ_s6=0.81\n",
      "- Regra 3: centro[age≈-0.92σ, sex≈-0.44σ, bmi≈-0.84σ, bp≈-0.80σ, s1≈-1.00σ, s2≈-0.96σ, s3≈+0.35σ, s4≈-0.87σ, s5≈-0.90σ, s6≈-0.83σ] | σ_age=0.89, σ_sex=0.86, σ_bmi=0.71, σ_bp=0.72, σ_s1=0.69, σ_s2=0.68, σ_s3=0.78, σ_s4=0.53, σ_s5=0.64, σ_s6=0.83\n",
      "- Regra 4: centro[age≈+0.28σ, sex≈-0.27σ, bmi≈+0.54σ, bp≈+0.48σ, s1≈+0.26σ, s2≈+0.18σ, s3≈-0.17σ, s4≈+0.22σ, s5≈+0.48σ, s6≈+0.46σ] | σ_age=0.83, σ_sex=0.95, σ_bmi=0.94, σ_bp=0.99, σ_s1=0.85, σ_s2=0.86, σ_s3=0.86, σ_s4=0.82, σ_s5=0.87, σ_s6=0.89\n",
      "- Regra 5: centro[age≈+0.38σ, sex≈+0.47σ, bmi≈+0.49σ, bp≈+0.49σ, s1≈+0.22σ, s2≈+0.25σ, s3≈-0.55σ, s4≈+0.59σ, s5≈+0.57σ, s6≈+0.53σ] | σ_age=0.83, σ_sex=0.92, σ_bmi=0.91, σ_bp=0.94, σ_s1=0.86, σ_s2=0.87, σ_s3=0.77, σ_s4=0.84, σ_s5=0.86, σ_s6=0.90\n",
      "- Regra 6: centro[age≈-0.03σ, sex≈-0.58σ, bmi≈-0.25σ, bp≈-0.24σ, s1≈-0.03σ, s2≈-0.17σ, s3≈+0.75σ, s4≈-0.60σ, s5≈-0.42σ, s6≈-0.35σ] | σ_age=0.87, σ_sex=0.77, σ_bmi=0.80, σ_bp=0.85, σ_s1=0.77, σ_s2=0.74, σ_s3=0.98, σ_s4=0.63, σ_s5=0.73, σ_s6=0.83\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataclasses import dataclass\n",
    "from typing import Tuple\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import (\n",
    "    mean_squared_error,\n",
    "    accuracy_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    ")\n",
    "from sklearn.datasets import load_diabetes, fetch_openml\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "# =========================\n",
    "# CONFIG  \n",
    "# =========================\n",
    "TASK = \"regression\"   # \"regression\" ou \"classification\"\n",
    "DATASET = \"sklearn_diabetes\"   # \"sklearn_diabetes\" | \"pima_openml\" | \"excel\"\n",
    "EXCEL_PATH = \"data.xlsx\"  # se DATASET=\"excel\", apontar para o ficheiro\n",
    "TARGET_COL = \"target\"     # nome da coluna target para o Excel\n",
    "\n",
    "N_CLUSTERS = 6            # nº de regras / clusters \n",
    "M_FCM = 1.6               # fuzzifier \n",
    "TEST_SIZE = 0.2\n",
    "RANDOM_STATE = 42\n",
    "# =========================\n",
    "\n",
    "# -------- utils ----------\n",
    "def _to_numpy(x):\n",
    "    return x.detach().cpu().numpy() if isinstance(x, torch.Tensor) else x\n",
    "\n",
    "def _weighted_mean_std(Xz: np.ndarray, U: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    \"\"\"\n",
    "    Estima média (centro) e sigma por regra/feature com pesos U^m (consistentes com FCM).\n",
    "    Retorna centers (R,D) e sigmas (R,D).\n",
    "    \"\"\"\n",
    "    R, N = U.shape\n",
    "    D = Xz.shape[1]\n",
    "    Um = U ** M_FCM\n",
    "    centers = np.zeros((R, D))\n",
    "    sigmas = np.zeros((R, D))\n",
    "    for r in range(R):\n",
    "        w = Um[r][:, None]  # (N,1)\n",
    "        mu = (w * Xz).sum(axis=0) / (w.sum(axis=0) + 1e-12)\n",
    "        centers[r] = mu\n",
    "        # var ponderada\n",
    "        var = (w * (Xz - mu) ** 2).sum(axis=0) / (w.sum(axis=0) + 1e-12)\n",
    "        sigmas[r] = np.sqrt(var + 1e-6)  # evitar sigma=0\n",
    "    return centers, sigmas\n",
    "\n",
    "def _design_matrix(Xz: np.ndarray, centers: np.ndarray, sigmas: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Constroi Phi para TSK (ordem 1): para cada amostra i,\n",
    "    concatena para cada regra r:  [w_r_normalizada(i), w_r_normalizada(i)*x(i)]\n",
    "    (i.e., b_r e W_r partilham a mesma ponderação normalizada).\n",
    "    Resultado: Phi shape (N, R*(1+D))\n",
    "    \"\"\"\n",
    "    R, D = centers.shape\n",
    "    N = Xz.shape[0]\n",
    "    # Gaussian MFs por feature\n",
    "    # w_r(i) = prod_d exp(-0.5 * ((x_id - c_rd)/sigma_rd)^2)\n",
    "    # Para estabilidade, somar logs:\n",
    "    exps = []\n",
    "    for r in range(R):\n",
    "        z = (Xz - centers[r]) / (sigmas[r] + 1e-12)  # (N,D)\n",
    "        log_phi = -0.5 * (z ** 2).sum(axis=1)        # (N,)\n",
    "        exps.append(log_phi)\n",
    "    log_w = np.stack(exps, axis=1)  # (N,R)\n",
    "    # normalizar por regra para cada amostra\n",
    "    # w_norm = softmax(log_w) sem “temperatura”\n",
    "    maxlog = np.max(log_w, axis=1, keepdims=True)\n",
    "    w = np.exp(log_w - maxlog)\n",
    "    w = w / (w.sum(axis=1, keepdims=True) + 1e-12)  # (N,R) normalizado\n",
    "\n",
    "    # Construir Phi\n",
    "    Phi_parts = []\n",
    "    ones = np.ones((N, 1))\n",
    "    for r in range(R):\n",
    "        wr = w[:, [r]]  # (N,1)\n",
    "        Phi_r = np.hstack([wr * ones, wr * Xz])  # (N, 1+D)\n",
    "        Phi_parts.append(Phi_r)\n",
    "    Phi = np.hstack(Phi_parts)  # (N, R*(1+D))\n",
    "    return Phi, w\n",
    "\n",
    "# ------- Modelo TSK -------\n",
    "@dataclass\n",
    "class TSKModel(nn.Module):\n",
    "    centers: np.ndarray  # (R,D)\n",
    "    sigmas: np.ndarray   # (R,D)\n",
    "    D: int\n",
    "    R: int\n",
    "\n",
    "    def __post_init__(self):\n",
    "        super().__init__()\n",
    "        self.D = self.centers.shape[1]\n",
    "        self.R = self.centers.shape[0]\n",
    "        # Parâmetros consequentes (empilhados): para cada regra r: [b_r, w_r1, ..., w_rD]\n",
    "        # Inicializa zeros; serão aprendidos por LS\n",
    "        self.theta = nn.Parameter(torch.zeros(self.R, self.D + 1), requires_grad=False)\n",
    "\n",
    "    def forward(self, Xz: torch.Tensor):\n",
    "        \"\"\"\n",
    "        Xz: (N,D) padronizado\n",
    "        Retorna:\n",
    "          y_pred: (N,1)\n",
    "          w_norm: (N,R) firing strengths normalizados\n",
    "          Phi:    (N,R*(1+D)) design matrix usada no LS\n",
    "        \"\"\"\n",
    "        X = Xz  # (N,D)\n",
    "        N = X.shape[0]\n",
    "        # computar w_norm e Phi em numpy (mais simples) e converter\n",
    "        Phi_np, w_norm_np = _design_matrix(_to_numpy(X), self.centers, self.sigmas)\n",
    "        Phi = torch.from_numpy(Phi_np).to(dtype=torch.float32, device=X.device)       # (N, R*(1+D))\n",
    "        w_norm = torch.from_numpy(w_norm_np).to(dtype=torch.float32, device=X.device) # (N,R)\n",
    "\n",
    "        # y = sum_r ( w_norm_r * (b_r + w_r^T x) )\n",
    "        # Podemos obter y via Phi @ vec(theta)\n",
    "        theta_vec = self.theta.reshape(-1)  # (R*(1+D),)\n",
    "        y = Phi @ theta_vec  # (N,)\n",
    "        return y.view(-1, 1), w_norm, Phi\n",
    "\n",
    "# ------ Least Squares ------\n",
    "def train_ls(model: TSKModel, Xz: np.ndarray, y: np.ndarray, task: str):\n",
    "    \"\"\"\n",
    "    Ajusta theta por LS:\n",
    "      theta = (Phi^T Phi)^(-1) Phi^T y\n",
    "    Para classificação, ajusta LS no espaço do 'logit' (aproximação):\n",
    "      y_tilde = log(p/(1-p))  com clipping p∈[1e-3, 1-1e-3]\n",
    "    \"\"\"\n",
    "    device = torch.device(\"cpu\")\n",
    "    model.to(device)\n",
    "    Xz_t = torch.from_numpy(Xz.astype(np.float32))\n",
    "    y_t = torch.from_numpy(y.astype(np.float32)).view(-1, 1)\n",
    "\n",
    "    # Para classificação: transformar rótulos (0/1) em valores-alvo contínuos via logit\n",
    "    if task == \"classification\":\n",
    "        p = y_t.clamp(1e-3, 1 - 1e-3)  # evita inf\n",
    "        y_ls = torch.log(p / (1 - p))\n",
    "    else:\n",
    "        y_ls = y_t\n",
    "\n",
    "    # Obter Phi\n",
    "    with torch.no_grad():\n",
    "        _, _, Phi = model(Xz_t)  # (N, R*(1+D))\n",
    "\n",
    "    # Resolver LS: theta = (Phi^T Phi + λI)^(-1) Phi^T y\n",
    "    lam = 1e-6\n",
    "    A = Phi.T @ Phi + lam * torch.eye(Phi.shape[1])\n",
    "    b = Phi.T @ y_ls\n",
    "    # >>> FIX AQUI: usar torch.linalg.solve <<<\n",
    "    theta_vec = torch.linalg.solve(A, b)\n",
    "    theta = theta_vec.view(model.R, model.D + 1)\n",
    "    with torch.no_grad():\n",
    "        model.theta.copy_(theta)\n",
    "\n",
    "\n",
    "# --------- Dados ----------\n",
    "def load_data():\n",
    "    if DATASET == \"sklearn_diabetes\":\n",
    "        # REGRESSÃO\n",
    "        ds = load_diabetes()\n",
    "        X = ds.data.astype(float)\n",
    "        y = ds.target.astype(float)\n",
    "        names = list(ds.feature_names)\n",
    "        return X, y, names\n",
    "\n",
    "    elif DATASET == \"pima_openml\":\n",
    "        # CLASSIFICAÇÃO\n",
    "        df = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
    "        X_df = df.data.copy()\n",
    "        # corrigir zeros impossíveis e imputar\n",
    "        zero_bad = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "        for c in zero_bad:\n",
    "            if c in X_df.columns:\n",
    "                X_df[c] = X_df[c].replace(0, np.nan)\n",
    "        X_df = X_df.fillna(X_df.median(numeric_only=True))\n",
    "        # target string -> binário\n",
    "        y_ser = df.target.astype(str).str.strip().str.lower()\n",
    "        y = y_ser.isin([\"tested_positive\", \"positive\", \"pos\", \"1\", \"true\", \"yes\"]).astype(int).to_numpy()\n",
    "        X = X_df.to_numpy().astype(float)\n",
    "        names = list(X_df.columns)\n",
    "        return X, y, names\n",
    "\n",
    "    elif DATASET == \"excel\":\n",
    "        # Lê de Excel (última coluna = target, a não ser que TARGET_COL esteja definido)\n",
    "        X_df = pd.read_excel(EXCEL_PATH)\n",
    "        if TARGET_COL in X_df.columns:\n",
    "            y = X_df[TARGET_COL].to_numpy()\n",
    "            X_df = X_df.drop(columns=[TARGET_COL])\n",
    "        else:\n",
    "            # assume última coluna é o target\n",
    "            y = X_df.iloc[:, -1].to_numpy()\n",
    "            X_df = X_df.iloc[:, :-1]\n",
    "        X = X_df.to_numpy().astype(float)\n",
    "        names = list(X_df.columns)\n",
    "        return X, y, names\n",
    "\n",
    "    else:\n",
    "        raise ValueError(\"DATASET inválido. Use 'sklearn_diabetes', 'pima_openml' ou 'excel'.\")\n",
    "\n",
    "# --------- Main -----------\n",
    "def main():\n",
    "    X, y, feat_names = load_data()\n",
    "\n",
    "    # Força coerência com TASK\n",
    "    if TASK == \"regression\":\n",
    "        y = y.astype(float)\n",
    "    else:\n",
    "        # garante 0/1\n",
    "        y = (y > 0).astype(int)\n",
    "\n",
    "    # Escalonamento\n",
    "    scaler = StandardScaler().fit(X)\n",
    "    Xz = scaler.transform(X)\n",
    "\n",
    "    # Split train/test\n",
    "    Xtr, Xte, ytr, yte = train_test_split(\n",
    "        Xz, y, test_size=TEST_SIZE, random_state=RANDOM_STATE, stratify=(y if TASK==\"classification\" else None)\n",
    "    )\n",
    "\n",
    "    # FCM sobre treino (em espaço escalonado)\n",
    "    centers, U, *_ = fuzz.cluster.cmeans(\n",
    "        data=Xtr.T, c=N_CLUSTERS, m=M_FCM, error=1e-5, maxiter=300, init=None, seed=RANDOM_STATE\n",
    "    )  # centers: (R,D), U: (R,Ntr)\n",
    "\n",
    "    # Estimar sigmas ponderados\n",
    "    centers_w, sigmas_w = _weighted_mean_std(Xtr, U)    # (R,D), (R,D)\n",
    "    # Usa centers do FCM + sigmas ponderadas\n",
    "    centers_use = centers\n",
    "    sigmas_use = sigmas_w\n",
    "\n",
    "    # Construir modelo TSK\n",
    "    R, D = centers_use.shape\n",
    "    model = TSKModel(centers=centers_use, sigmas=sigmas_use, D=D, R=R)\n",
    "\n",
    "    # Treino por LS (fecho analítico)\n",
    "    train_ls(model, Xtr, ytr, TASK)\n",
    "\n",
    "    # Avaliação\n",
    "    y_pred_tr, _, _ = model(torch.from_numpy(Xtr.astype(np.float32)))\n",
    "    y_pred_te, _, _ = model(torch.from_numpy(Xte.astype(np.float32)))\n",
    "\n",
    "    if TASK == \"regression\":\n",
    "        yhat_te = _to_numpy(y_pred_te).ravel()\n",
    "        mse = mean_squared_error(yte, yhat_te)\n",
    "        print(f\"[REG] Test MSE: {mse:.3f}\")\n",
    "    else:\n",
    "        # para classificação, aplicar sigmoid ao output TSK (logit aproximado)\n",
    "        yhat_proba_te = 1 / (1 + np.exp(-_to_numpy(y_pred_te).ravel()))\n",
    "        yhat_te = (yhat_proba_te >= 0.5).astype(int)\n",
    "        acc = accuracy_score(yte, yhat_te)\n",
    "        f1 = f1_score(yte, yhat_te)\n",
    "        try:\n",
    "            auc = roc_auc_score(yte, yhat_proba_te)\n",
    "        except Exception:\n",
    "            auc = float(\"nan\")\n",
    "        print(f\"[CLS] Test Acc: {acc:.3f} | F1: {f1:.3f} | ROC-AUC: {auc:.3f}\")\n",
    "\n",
    "    # Info de regras (centros/sigmas em z-score)\n",
    "    print(\"\\nRegras (centros/sigmas em espaço padronizado):\")\n",
    "    for r in range(R):\n",
    "        c_txt = \", \".join([f\"{feat_names[d]}≈{centers_use[r,d]:+.2f}σ\" for d in range(D)])\n",
    "        s_txt = \", \".join([f\"σ_{feat_names[d]}={sigmas_use[r,d]:.2f}\" for d in range(D)])\n",
    "        print(f\"- Regra {r+1}: centro[{c_txt}] | {s_txt}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c4d27c4-886b-4d4f-a26b-cad6270a6e28",
   "metadata": {},
   "source": [
    "The regression task was conducted on the Diabetes dataset from scikit-learn, with 442 samples and 10 clinical features. A TSK fuzzy model was constructed using Fuzzy C-Means clustering to define rule antecedents and least-squares estimation for the consequents.\n",
    "\n",
    "The model achieved a Test MSE of 2443.032.The Mean Squared Error (MSE) represents the average of the squared differences between predicted and true values. Lower MSE values indicate better predictive accuracy, and the obtained result confirms that the fuzzy TSK model can capture relevant patterns in the data.\n",
    "Also, the rules obtained illustrate how the fuzzy system partitions the feature space into interpretable regions, each associated with a local regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1354e1ef-956b-454f-8354-1e933bdfa1d7",
   "metadata": {},
   "source": [
    "# Dataset 2: Pima Indians Diabetes Dataset (Classification)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8413ed36-50a5-4587-8c26-5ce4f7be5c57",
   "metadata": {},
   "source": [
    "For the classification task, we used the Pima Indians Diabetes dataset from OpenML, which contains 768 samples with 8 clinical features. A TSK fuzzy model was again applied, following the same approach as in the regression case: Fuzzy C-Means clustering for antecedents and least-squares estimation for consequents.\n",
    "\n",
    "Using this method, we obtained the following performance on the test set:\n",
    "\n",
    "[CLS] Test Acc: 0.734 | F1: 0.631 | ROC-AUC: 0.794\n",
    "\n",
    "Regras (centros/sigmas em espaço padronizado):\n",
    "- Regra 1: centro[preg≈+0.01σ, plas≈-0.06σ, pres≈+0.04σ, skin≈-0.08σ, insu≈-0.13σ, mass≈-0.03σ, pedi≈+0.01σ, age≈-0.02σ] | σ_preg=0.91, σ_plas=0.94, σ_pres=0.96, σ_skin=0.96, σ_insu=0.84, σ_mass=0.94, σ_pedi=0.99, σ_age=0.89\n",
    "- Regra 2: centro[preg≈-0.37σ, plas≈+0.30σ, pres≈+0.12σ, skin≈+0.67σ, insu≈+0.59σ, mass≈+0.49σ, pedi≈+0.34σ, age≈-0.27σ] | σ_preg=0.80, σ_plas=0.92, σ_pres=0.83, σ_skin=0.83, σ_insu=1.02, σ_mass=0.90, σ_pedi=1.05, σ_age=0.77\n",
    "- Regra 3: centro[preg≈-0.48σ, plas≈-0.56σ, pres≈-0.30σ, skin≈-0.38σ, insu≈-0.34σ, mass≈-0.72σ, pedi≈-0.26σ, age≈-0.61σ] | σ_preg=0.63, σ_plas=0.72, σ_pres=0.85, σ_skin=0.70, σ_insu=0.55, σ_mass=0.83, σ_pedi=0.74, σ_age=0.59\n",
    "- Regra 4: centro[preg≈+0.76σ, plas≈+0.71σ, pres≈+0.29σ, skin≈+0.49σ, insu≈+0.53σ, mass≈+0.24σ, pedi≈+0.36σ, age≈+0.80σ] | σ_preg=1.00, σ_plas=0.95, σ_pres=0.82, σ_skin=0.88, σ_insu=1.11, σ_mass=0.87, σ_pedi=1.05, σ_age=0.94\n",
    "- Regra 5: centro[preg≈+0.78σ, plas≈+0.23σ, pres≈+0.26σ, skin≈-0.90σ, insu≈-0.53σ, mass≈-0.09σ, pedi≈-0.32σ, age≈+0.99σ] | σ_preg=0.92, σ_plas=0.89, σ_pres=0.86, σ_skin=0.79, σ_insu=0.59, σ_mass=0.88, σ_pedi=0.80, σ_age=0.98\n",
    "- Regra 6: centro[preg≈-0.44σ, plas≈-0.34σ, pres≈-0.07σ, skin≈+0.40σ, insu≈-0.01σ, mass≈+0.21σ, pedi≈-0.06σ, age≈-0.48σ] | σ_preg=0.71, σ_plas=0.82, σ_pres=0.81, σ_skin=0.79, σ_insu=0.75, σ_mass=0.84, σ_pedi=0.87, σ_age=0.68\n",
    "\n",
    "To further explore possible improvements, I experimented with an alternative version of the model in which the consequents are estimated using logistic regression per rule rather than least squares. This adjustment increased accuracy and ROC-AUC, as presented in the following section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "7fe74d05-a08b-406d-9983-0ad00417dfef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best CV (ROC-AUC): 0.834 with R=6 and m=1.6\n",
      "Test Acc: 0.799 | F1: 0.674 | ROC-AUC: 0.865\n",
      "\n",
      "Rules (standardized space):\n",
      " - Rule 1: IF x near center[preg≈-0.23σ, plas≈0.65σ, pres≈0.15σ, skin≈0.62σ, insu≈0.91σ, mass≈0.49σ, pedi≈0.25σ, age≈-0.19σ] THEN output via logistic model (see weights).\n",
      " - Rule 2: IF x near center[preg≈0.95σ, plas≈0.53σ, pres≈0.34σ, skin≈0.40σ, insu≈0.25σ, mass≈0.25σ, pedi≈0.26σ, age≈0.79σ] THEN output via logistic model (see weights).\n",
      " - Rule 3: IF x near center[preg≈0.72σ, plas≈0.26σ, pres≈0.30σ, skin≈-0.94σ, insu≈-0.53σ, mass≈-0.11σ, pedi≈-0.26σ, age≈1.04σ] THEN output via logistic model (see weights).\n",
      " - Rule 4: IF x near center[preg≈-0.19σ, plas≈-0.17σ, pres≈-0.09σ, skin≈-0.16σ, insu≈-0.14σ, mass≈-0.14σ, pedi≈0.02σ, age≈-0.23σ] THEN output via logistic model (see weights).\n",
      " - Rule 5: IF x near center[preg≈-0.47σ, plas≈-0.35σ, pres≈-0.06σ, skin≈0.55σ, insu≈0.03σ, mass≈0.35σ, pedi≈-0.06σ, age≈-0.48σ] THEN output via logistic model (see weights).\n",
      " - Rule 6: IF x near center[preg≈-0.47σ, plas≈-0.62σ, pres≈-0.35σ, skin≈-0.35σ, insu≈-0.35σ, mass≈-0.70σ, pedi≈-0.30σ, age≈-0.61σ] THEN output via logistic model (see weights).\n",
      "\n",
      "Artifacts saved to: C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignement1\\artifacts\n"
     ]
    }
   ],
   "source": [
    "# classification_pima_tsk.py\n",
    "# TSK fuzzy classification on Pima Indians dataset via OpenML\n",
    "# Usage: python classification_pima_tsk.py\n",
    "# Dependencies: numpy, pandas, scikit-learn, scikit-fuzzy, matplotlib\n",
    "\n",
    "import os, json, numpy as np, pandas as pd, matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "from sklearn.datasets import fetch_openml\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score, roc_curve\n",
    "import skfuzzy as fuzz\n",
    "\n",
    "ARTIFACT_DIR = r\"C:\\Users\\alexa\\Desktop\\Ist100514\\si\\assignement1\\artifacts\"\n",
    "os.makedirs(ARTIFACT_DIR, exist_ok=True)\n",
    "rng = np.random.default_rng(42)\n",
    "\n",
    "def fcm_train(X, n_rules, m=2.0, max_iter=300, error=1e-5, seed=42):\n",
    "    cntr, U, _, _, _, _, _ = fuzz.cluster.cmeans(\n",
    "        data=X.T, c=n_rules, m=m, error=error, maxiter=max_iter, init=None, seed=seed\n",
    "    )\n",
    "    return cntr, U\n",
    "\n",
    "def fcm_membership_for_new(X, centers, m=2.0, eps=1e-12):\n",
    "    n_rules = centers.shape[0]; n_samples = X.shape[0]\n",
    "    d = np.zeros((n_rules, n_samples))\n",
    "    for r in range(n_rules):\n",
    "        diff = X - centers[r]\n",
    "        d[r] = np.linalg.norm(diff, axis=1) + eps\n",
    "    power = 2.0/(m-1.0)\n",
    "    denom = np.zeros_like(d)\n",
    "    for r in range(n_rules):\n",
    "        denom[r] = np.sum((d[r][:,None] / d.T)**power, axis=1)\n",
    "    return 1.0/denom\n",
    "\n",
    "@dataclass\n",
    "class TSKClassifier:\n",
    "    n_rules: int\n",
    "    m: float = 2.0\n",
    "    centers_: np.ndarray = None\n",
    "    clfs_: List[LogisticRegression] = None\n",
    "    scaler_: StandardScaler = None\n",
    "    feature_names_: List[str] = None\n",
    "\n",
    "    def fit(self, X: np.ndarray, y: np.ndarray, feature_names=None):\n",
    "        self.feature_names_ = feature_names or [f\"x{i}\" for i in range(X.shape[1])]\n",
    "        self.scaler_ = StandardScaler().fit(X)\n",
    "        Xs = self.scaler_.transform(X)\n",
    "        centers, U = fcm_train(Xs, self.n_rules, self.m, seed=42)\n",
    "        self.centers_ = centers\n",
    "        self.clfs_ = []\n",
    "        for r in range(self.n_rules):\n",
    "            w = (U[r]**self.m)\n",
    "            clf = LogisticRegression(max_iter=400, solver=\"lbfgs\")\n",
    "            clf.fit(Xs, y, sample_weight=w)\n",
    "            self.clfs_.append(clf)\n",
    "        return self\n",
    "\n",
    "    def predict_proba(self, X: np.ndarray) -> np.ndarray:\n",
    "        Xs = self.scaler_.transform(X)\n",
    "        U = fcm_membership_for_new(Xs, self.centers_, self.m)\n",
    "        w = (U**self.m)\n",
    "        pr = np.stack([clf.predict_proba(Xs)[:,1] for clf in self.clfs_], axis=0)\n",
    "        p = np.sum(w * pr, axis=0) / np.sum(w, axis=0)\n",
    "        return np.vstack([1-p, p]).T\n",
    "\n",
    "    def predict(self, X: np.ndarray) -> np.ndarray:\n",
    "        return (self.predict_proba(X)[:,1] >= 0.5).astype(int)\n",
    "\n",
    "    def pretty_rules(self) -> List[str]:\n",
    "        rules = []\n",
    "        for r, c in enumerate(self.centers_):\n",
    "            center = \", \".join([f\"{name}≈{c[i]:.2f}σ\" for i, name in enumerate(self.feature_names_)])\n",
    "            rules.append(f\"Rule {r+1}: IF x near center[{center}] THEN output via logistic model (see weights).\")\n",
    "        return rules\n",
    "\n",
    "def grid_search_tsk_clf(X, y, n_rules_grid=(2,3,4,5,6), m_grid=(1.6,2.0,2.4), cv=5, random_state=42):\n",
    "    kf = KFold(n_splits=cv, shuffle=True, random_state=random_state)\n",
    "    best = {\"auc\": -np.inf}\n",
    "    for R in n_rules_grid:\n",
    "        for m in m_grid:\n",
    "            aucs = []\n",
    "            for tr, va in kf.split(X):\n",
    "                model = TSKClassifier(n_rules=R, m=m).fit(X[tr], y[tr])\n",
    "                proba = model.predict_proba(X[va])[:,1]\n",
    "                try:\n",
    "                    aucs.append(roc_auc_score(y[va], proba))\n",
    "                except ValueError:\n",
    "                    aucs.append(0.5)\n",
    "            auc = float(np.mean(aucs))\n",
    "            if auc > best[\"auc\"]:\n",
    "                best = {\"auc\": auc, \"n_rules\": R, \"m\": m}\n",
    "    return best\n",
    "\n",
    "def main():\n",
    "    # Load Pima from OpenML (id=37, name='diabetes')\n",
    "    ds = fetch_openml(name=\"diabetes\", version=1, as_frame=True)\n",
    "\n",
    "    # --- Robust label mapping (strings -> 0/1) ---\n",
    "    y_series = ds.target.astype(str).str.strip().str.lower()\n",
    "    # True/positive bucket covers common variants\n",
    "    y = y_series.isin([\"tested_positive\", \"positive\", \"pos\", \"1\", \"true\", \"yes\"]).astype(int).to_numpy()\n",
    "\n",
    "    # --- Optional: clean physiologically invalid zeros and impute ---\n",
    "    X_df = ds.data.copy()\n",
    "    zero_bad = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\n",
    "    for c in zero_bad:\n",
    "        if c in X_df.columns:\n",
    "            X_df[c] = X_df[c].replace(0, np.nan)\n",
    "    X_df = X_df.fillna(X_df.median(numeric_only=True))\n",
    "\n",
    "    names = list(X_df.columns)\n",
    "    X = X_df.to_numpy().astype(float)\n",
    "\n",
    "    # Hyperparam search for TSK\n",
    "    best = grid_search_tsk_clf(X, y)\n",
    "    print(f\"Best CV (ROC-AUC): {best['auc']:.3f} with R={best['n_rules']} and m={best['m']}\")\n",
    "\n",
    "    # 80/20 split (reproducible)\n",
    "    n = X.shape[0]\n",
    "    idx = np.random.default_rng(42).permutation(n)\n",
    "    split = int(0.8*n)\n",
    "    tr, te = idx[:split], idx[split:]\n",
    "\n",
    "    model = TSKClassifier(n_rules=best['n_rules'], m=best['m']).fit(X[tr], y[tr], feature_names=names)\n",
    "    proba = model.predict_proba(X[te])[:,1]\n",
    "    pred = (proba >= 0.5).astype(int)\n",
    "\n",
    "    acc = float(accuracy_score(y[te], pred))\n",
    "    f1  = float(f1_score(y[te], pred))\n",
    "    try:\n",
    "        auc = float(roc_auc_score(y[te], proba))\n",
    "    except ValueError:\n",
    "        auc = float('nan')\n",
    "    print(f\"Test Acc: {acc:.3f} | F1: {f1:.3f} | ROC-AUC: {auc:.3f}\\n\")\n",
    "    print(\"Rules (standardized space):\")\n",
    "    for s in model.pretty_rules(): print(\" -\", s)\n",
    "\n",
    "    # ROC curve\n",
    "    fpr, tpr, _ = roc_curve(y[te], proba)\n",
    "    plt.figure()\n",
    "    plt.plot(fpr, tpr, label=f\"TSK (AUC={auc:.3f})\")\n",
    "    plt.plot([0,1],[0,1], linestyle=\"--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curve – TSK Classifier (Pima)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(os.path.join(ARTIFACT_DIR, \"pima_roc_curve.png\"))\n",
    "    plt.close()\n",
    "\n",
    "    # Save artifacts\n",
    "    art = {\n",
    "        \"task\": \"classification_pima\",\n",
    "        \"best_cv\": best,\n",
    "        \"test_accuracy\": acc,\n",
    "        \"test_f1\": f1,\n",
    "        \"test_auc\": auc,\n",
    "        \"feature_names\": names,\n",
    "        \"centers\": model.centers_.tolist(),\n",
    "        \"per_rule_logreg\": [\n",
    "            {\"coef\": clf.coef_.ravel().tolist(), \"intercept\": float(clf.intercept_[0])}\n",
    "            for clf in model.clfs_\n",
    "        ],\n",
    "    }\n",
    "    with open(os.path.join(ARTIFACT_DIR, \"classification_artifacts.json\"), \"w\") as f:\n",
    "        json.dump(art, f, indent=2)\n",
    "    print(f\"\\nArtifacts saved to: {os.path.abspath(ARTIFACT_DIR)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7511be55-bd94-4e3f-a70b-65b19d475b7c",
   "metadata": {},
   "source": [
    "In addition to the teacher-style TSK model, I implemented a variant where the rule consequents are trained as weighted logistic regressions instead of least squares. Each fuzzy cluster defines a local region, and within that region a logistic regression is fitted using the cluster memberships as sample weights. At inference time, the per-rule probabilities are combined according to the normalized membership degrees.\n",
    "\n",
    "Comparison with Template derived Model\n",
    "\n",
    "Template approach (LS + sigmoid): Accuracy ≈ 0.734, ROC-AUC ≈ 0.794.\n",
    "\n",
    "Logistic TSK: Accuracy ≈ 0.799, ROC-AUC ≈ 0.834.\n",
    "\n",
    "The improvement arises because least squares regression is not optimal for classification: it minimizes squared error on 0/1 labels and only approximates the probability via a sigmoid. In contrast, logistic regression directly optimizes log-likelihood, leading to better separation between classes and more calibrated probabilities.\n",
    "\n",
    "Thus, while both methods retain the interpretability of fuzzy rules, the logistic-based TSK achieves higher predictive performance, particularly in terms of ROC-AUC and overall accuracy.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "471c2426-dfae-46d2-ba6e-5fba0d6e433c",
   "metadata": {},
   "source": [
    "## Discussion and Conclusion\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f9618aa-84a6-4825-b05e-ef5bfd7e4cca",
   "metadata": {},
   "source": [
    "This work applied TSK fuzzy systems to regression and classification tasks. On the Diabetes regression dataset, the model achieved a test MSE of 2443, showing that fuzzy rules can capture relevant relationships while remaining interpretable.\n",
    "\n",
    "For the Pima Indians classification dataset, the template-style model with least-squares consequents reached an accuracy of 0.734. An alternative variant with logistic regression consequents improved performance to accuracy 0.799 and ROC-AUC 0.834, demonstrating that adapting the consequent type to the task can enhance results.\n",
    "\n",
    "Overall, the experiments confirm that TSK fuzzy systems are effective and interpretable, and that model performance can be further improved with task-appropriate consequents."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
